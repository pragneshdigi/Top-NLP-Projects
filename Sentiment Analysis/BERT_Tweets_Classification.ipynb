{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23626,"status":"ok","timestamp":1651740725918,"user":{"displayName":"Pragnesh Solanki","userId":"07111032951740583629"},"user_tz":-330},"id":"A_CNet_BpcA3","outputId":"547f27dd-8a50-46f2-bcc6-fcf75c817cf2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.8.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.44.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (4.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.14.0)\n","Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.25.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow) (57.4.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n","Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.0)\n","Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.0.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n","Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n","Collecting tf-estimator-nightly==2.8.0.dev2021122109\n","  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n","\u001b[K     |████████████████████████████████| 462 kB 5.3 MB/s \n","\u001b[?25hRequirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n","Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (14.0.1)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.21.6)\n","Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.5.3)\n","Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.8.1)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.0.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.3.6)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.6.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.35.0)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.23.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.4.6)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.2.4)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.2.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (4.11.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.8.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.4.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2021.10.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (3.2.0)\n","Installing collected packages: tf-estimator-nightly\n","Successfully installed tf-estimator-nightly-2.8.0.dev2021122109\n","Requirement already satisfied: tensorflow_hub in /usr/local/lib/python3.7/dist-packages (0.12.0)\n","Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_hub) (3.17.3)\n","Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_hub) (1.21.6)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorflow_hub) (1.15.0)\n","Collecting bert-for-tf2\n","  Downloading bert-for-tf2-0.14.9.tar.gz (41 kB)\n","\u001b[K     |████████████████████████████████| 41 kB 105 kB/s \n","\u001b[?25hCollecting py-params>=0.9.6\n","  Downloading py-params-0.10.2.tar.gz (7.4 kB)\n","Collecting params-flow>=0.8.0\n","  Downloading params-flow-0.8.2.tar.gz (22 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.21.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.64.0)\n","Building wheels for collected packages: bert-for-tf2, params-flow, py-params\n","  Building wheel for bert-for-tf2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for bert-for-tf2: filename=bert_for_tf2-0.14.9-py3-none-any.whl size=30535 sha256=bb930ac1a0d79c0488fa8d4c046b1bac8ffeddc003380ac6ef95c06789c521f6\n","  Stored in directory: /root/.cache/pip/wheels/47/b6/e5/8c76ec779f54bc5c2f1b57d2200bb9c77616da83873e8acb53\n","  Building wheel for params-flow (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for params-flow: filename=params_flow-0.8.2-py3-none-any.whl size=19472 sha256=447ffb387c0469048b8649c59e1465844dfd3c0a0fbe389a16d8fc5ca8f0fdfd\n","  Stored in directory: /root/.cache/pip/wheels/0e/fc/d2/a44fff33af0f233d7def6e7de413006d57c10e10ad736fe8f5\n","  Building wheel for py-params (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for py-params: filename=py_params-0.10.2-py3-none-any.whl size=7911 sha256=047cfbf651500038474421255feb020f18508329619f40e6747b5d6c96d728bf\n","  Stored in directory: /root/.cache/pip/wheels/e1/11/67/33cc51bbee127cb8fb2ba549cd29109b2f22da43ddf9969716\n","Successfully built bert-for-tf2 params-flow py-params\n","Installing collected packages: py-params, params-flow, bert-for-tf2\n","Successfully installed bert-for-tf2-0.14.9 params-flow-0.8.2 py-params-0.10.2\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 5.2 MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.96\n","Tensorflow Version:  2.8.0\n"]}],"source":["# Reference\n","# https://colab.research.google.com/drive/1ARH9dnugVuKjRTNorKIVrgRKitjg051c?usp=sharing#scrollTo=DAGIEVsCpuc1\n","\n","!pip install tensorflow\n","!pip install tensorflow_hub\n","!pip install bert-for-tf2\n","!pip install sentencepiece\n","\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from tensorflow.keras.layers import Dense, Input, Dropout\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","import tensorflow_hub as hub\n","from bert import bert_tokenization\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","print(\"Tensorflow Version: \", tf.__version__)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1651733036856,"user":{"displayName":"Pragnesh Solanki","userId":"07111032951740583629"},"user_tz":-330},"id":"NVVt8XWwscve","outputId":"16c130e3-7dcb-46e9-beb0-1c8d19eb11dc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Data Shape: (7613, 2)\n"]}],"source":["df = pd.read_csv('https://raw.githubusercontent.com/sanigam/BERT_Medium/master/tweets.csv')\n","\n","print (f'Data Shape: {df.shape}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":603},"executionInfo":{"elapsed":956,"status":"ok","timestamp":1651733037796,"user":{"displayName":"Pragnesh Solanki","userId":"07111032951740583629"},"user_tz":-330},"id":"drjS6fwg1QYG","outputId":"00c0619d-0f16-4ba9-a1bc-59ba8bf05e4a"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-909e020d-2f9c-4eb2-8ed4-df8023ef8fe4\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>target</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>6917</th>\n","      <td>@ChubbySquirrel_ @Hurricane_Surge this here is...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>5010</th>\n","      <td>@crabbycale OH MY GOD THE MEMORIES ARE FLOODIN...</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-909e020d-2f9c-4eb2-8ed4-df8023ef8fe4')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-909e020d-2f9c-4eb2-8ed4-df8023ef8fe4 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-909e020d-2f9c-4eb2-8ed4-df8023ef8fe4');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["                                                   text  target\n","6917  @ChubbySquirrel_ @Hurricane_Surge this here is...       1\n","5010  @crabbycale OH MY GOD THE MEMORIES ARE FLOODIN...       0"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","Sample Non Disaster tweet (target = 0):\n","-------------------------------\n","Listen to this hit song. A summer Bomb full of positive energy and youth\n","Did you like it?\n","https://t.co/2LiWkJybE9 \n","#Norge2040\n","\n","Sample Tweet indicating Disaster (target = 1):\n","--------------------------------------\n","Still can't get over the thunderstorm/tornado we were woken up to yesterday. Half the street is still in the dark! http://t.co/Y8h5v1j2y7\n","\n","Tweets distribution for Disaster Tweets (1)  and Non-Disaster Tweets (0)\n","------------------------------------------------------------------------\n"]},{"data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["#Optional Step: Looking into data\n","display(df.sample(2)) #Sample rows of dataframe\n","\n","print ( '\\nSample Non Disaster tweet (target = 0):\\n-------------------------------')\n","print ( df[df['target']==0].text.values[0] )\n","\n","print ( '\\nSample Tweet indicating Disaster (target = 1):\\n--------------------------------------')\n","print ( df[df['target']==1].text.values[0] )\n","\n","print ( '\\nTweets distribution for Disaster Tweets (1)  and Non-Disaster Tweets (0)\\n------------------------------------------------------------------------')\n","df['target'].hist() ;"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1651733037797,"user":{"displayName":"Pragnesh Solanki","userId":"07111032951740583629"},"user_tz":-330},"id":"NMfftauc2ouW","outputId":"bb49e9d6-9a4b-414e-c240-a268a7f431a9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training data shape: (5100, 2) , Test data shape: (2513, 2)\n"]}],"source":["df_train, df_test = train_test_split(df, test_size=0.33, random_state=42)\n","\n","print( f'Training data shape: {df_train.shape} , Test data shape: {df_test.shape}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32482,"status":"ok","timestamp":1651733070273,"user":{"displayName":"Pragnesh Solanki","userId":"07111032951740583629"},"user_tz":-330},"id":"ZbPz3Zyq4LeD","outputId":"3636a982-8193-4b22-8711-ed76d2f165f7"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Length of vocab in our tokenizer :  30522\n"]}],"source":["#Loading BERT Standard model (Pretrained Model on Wikipedia and Book Corpus)\n","bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",trainable=True, name = 'keras_bert_layer' )\n","\n","#Getting vocab file from bert layer\n","vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy() \n","\n","do_lower_case = True  # For uncased model it will True \n","\n","#Defining tokenizer object which will be used to tokenize text before feeding to bert\n","tokenizer_for_bert = bert_tokenization.FullTokenizer(vocab_file, do_lower_case) #Tokenizer to tokenize input text\n","\n","print ( '\\nLength of vocab in our tokenizer : ' , len(tokenizer_for_bert.vocab) ) #BERT vocab has around 30K words"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6EjFGJdl4Zku"},"outputs":[],"source":["#Function to encode text in format to feed to BERT\n","\n","def encode_text_for_bert (texts, tokenizer_for_bert, max_len=512):\n","    ''' This function is to encode data for inputting into BERT model\n","    Parameters:\n","    texts - List of texts to encode\n","    tokenizer_for_bert - Tokenizer to be used to convert text into tokens\n","    max_len - Maximum length of text. It can have maximum value as 512\n","    Return: Tupple of 3 numpy arrays \n","    1) Token Ids padded with 0s to make length as max_len.  \n","    2) Array where we have 1 for actual tokens and 0 for padding tokens\n","    3) Array of 0s to indicate that token belongs to 1st sentence (chunk of text). There is no 2nd sentence here.\n","    '''\n","    all_token_ids = []\n","    all_masks = []\n","    all_segments = []\n","    \n","    for text in texts:\n","        tokens = tokenizer_for_bert.tokenize(text) #Tokenizing using Bert tokenizer\n","            \n","        tokens = tokens[:max_len-2] # Truncating number of tokens to max_len -2, Reduced extra 2 to add special tokens\n","        \n","        input_sequence = [\"[CLS]\"] + tokens + [\"[SEP]\"]  # [CLS] and [SEP] are special tokens to be added into input text\n","        \n","        pad_len = max_len - len(input_sequence) # Spaces to fill with 0s to make each sequence equal to max_len\n","        \n","        token_ids = tokenizer_for_bert.convert_tokens_to_ids(input_sequence)   #Converting tokens to token ids \n","       \n","        token_ids += [0] * pad_len  #Padding token ids with 0s\n","        \n","        pad_masks = [1] * len(input_sequence) + [0] * pad_len # 1 where we have sentence tokens and 0 otherwise\n","        \n","        segment_ids = [0] * max_len # Segment ids are all 0 to indicate it is part of sentence 1. There is no sentence 2 here\n","        \n","        all_token_ids.append(token_ids)\n","        all_masks.append(pad_masks)\n","        all_segments.append(segment_ids)\n","    \n","    return np.array(all_token_ids), np.array(all_masks), np.array(all_segments)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33,"status":"ok","timestamp":1651733070278,"user":{"displayName":"Pragnesh Solanki","userId":"07111032951740583629"},"user_tz":-330},"id":"lD6_7T-D48dC","outputId":"8b48b67d-1c82-4aff-9a0c-094d2b8a6283"},"outputs":[{"name":"stdout","output_type":"stream","text":["Test text after tokenization:  ['[CLS]', 'welcome', 'to', 'bert', 'session', '[SEP]']\n","Test text after encoding:  (array([[  101,  6160,  2000, 14324,  5219,   102,     0]]), array([[1, 1, 1, 1, 1, 1, 0]]), array([[0, 0, 0, 0, 0, 0, 0]]))\n"]}],"source":["#Optional Step: This is just to understand input/output of function encode_text_for_bert\n","test_text =  \"Welcome to  BERT session \"\n","\n","print (\"Test text after tokenization: \" ,  [\"[CLS]\"] + tokenizer_for_bert.tokenize( test_text)  + [\"[SEP]\"] )\n","\n","print (\"Test text after encoding: \" ,encode_text_for_bert ( [test_text], tokenizer_for_bert, 7 ) ) # Pl Note id 101 is for token [CLS] and 102 for token [SEP]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1kkg5cWD5Gd9"},"outputs":[],"source":["#Creating BERT  Model \n","def bert_model_creation (bert_layer, max_len=512, model_type = 'Classification', num_classes = 2):\n","    '''This function is to create BERT model for Classification or Regession Task\n","    Parameters:\n","    model_type = 'Classification' for classification task or 'Regression' for regression task. \n","    num_classes = Number of classes in classification task. Value of 2 means binary classification. More than 2 for multiclass classification.\n","                  For regression, num_classes parameter is ignored.\n","    Return: Deep Learning Model\n","    Important: You may add additional dense layers in place holder provided as \"***PLACEHOLDFER FOR ADDITIONAL LAYERS****\"\n","    '''   \n","    #Input to bert layer\n","    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n","    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n","    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n","\n","    #Output from bert layer\n","    bert_layer_out = bert_layer([input_word_ids, input_mask, segment_ids]) # Python list of 2 tensors with shape (batch_size, 768) and (batch_size, max_len, 768)\n","    \n","    #Extrating Embedding for CLS token comming out of bert layer. Note CLS is the first token\n","    cls_out = bert_layer_out[1][:,0,:] # Getting hidden-state of 1st tokens from second tensor in bert_layer_out, Tensor shape - (batch size, 768) \n","    \n","    \n","    #***PLACEHOLDFER FOR ADDITIONAL LAYERS****. \n","    #Add more layers here if you want. See example below\n","    #cls_out = Dropout(.25) (cls_out)\n","    #cls_out = Dense(500, activation='relu')(cls_out) \n","    \n","    \n","    \n","    #Defines last layer depending on model type and  number of classes. Activation function is used depending on model_type and num_classes\n","    if model_type == 'Classification' :\n","        if num_classes == 2 :\n","            out = Dense(1, activation='sigmoid')(cls_out)     # ** For Binary classification, use sigmoid activation\n","        else:    \n","            out = Dense(num_classes, activation='softmax')(cls_output) # For Multi Class classification, use softmax activation\n","    else:\n","        out = Dense(1, activation='linear')(cls_out)     # For regression, use linear activation\n","    \n","    #Model creation using inputs and output\n","    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out, name='deeplearning_bert__model')\n","    \n","    \n","    \n","    learning_rate = 2e-6 # modify learning rate,as needed\n","    \n","    #Compiles Model depending on model type and number of classes. Loss function as well as metrics is used accordingly\n","    if model_type == 'Classification' :\n","        if num_classes == 2 :\n","            model.compile(Adam(lr= learning_rate), loss='binary_crossentropy', metrics=['acc']) # ** For Binary classification\n","        else:\n","            model.compile(Adam(lr= learning_rate), loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy']) # For Multi Class classification \n","    else:\n","        model.compile(Adam(lr= learning_rate), loss='mse', metrics=['mse']) # For Regression\n","        \n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4BMaBqro5iDb"},"outputs":[],"source":["max_len = 30"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1568,"status":"ok","timestamp":1651733071822,"user":{"displayName":"Pragnesh Solanki","userId":"07111032951740583629"},"user_tz":-330},"id":"Lu-E_iqg5mMW","outputId":"2166fd6b-0b1c-43fe-905c-c32b4c496151"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"deeplearning_bert__model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_word_ids (InputLayer)    [(None, 30)]         0           []                               \n","                                                                                                  \n"," input_mask (InputLayer)        [(None, 30)]         0           []                               \n","                                                                                                  \n"," segment_ids (InputLayer)       [(None, 30)]         0           []                               \n","                                                                                                  \n"," keras_bert_layer (KerasLayer)  [(None, 768),        109482241   ['input_word_ids[0][0]',         \n","                                 (None, 30, 768)]                 'input_mask[0][0]',             \n","                                                                  'segment_ids[0][0]']            \n","                                                                                                  \n"," tf.__operators__.getitem (Slic  (None, 768)         0           ['keras_bert_layer[0][1]']       \n"," ingOpLambda)                                                                                     \n","                                                                                                  \n"," dense (Dense)                  (None, 1)            769         ['tf.__operators__.getitem[0][0]'\n","                                                                 ]                                \n","                                                                                                  \n","==================================================================================================\n","Total params: 109,483,010\n","Trainable params: 109,483,009\n","Non-trainable params: 1\n","__________________________________________________________________________________________________\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(Adam, self).__init__(name, **kwargs)\n"]}],"source":["#Building Classification Model\n","#modify values of model_type and num_classes as per need\n","model = bert_model_creation(bert_layer, max_len=max_len, model_type = 'Classification', num_classes = 2) #binary classification as num_classes = 2\n","\n","#Model Summary. Pl note, there are ~109 Million parameters as it is BERT standard model\n","model.summary() "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"psfigZhG5p5S"},"outputs":[],"source":["#Encoding Training Data for BERT.  If you want  preprocessing/cleaning of input text, it should be done before this step\n","train_input = encode_text_for_bert(df_train['text'].values, tokenizer_for_bert, max_len= max_len)\n","\n","#Output variable as 0s or 1s for binary classification. It can have more distinct values for multi-class classification or continous values for regression\n","y_train = df_train['target'].values"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28361,"status":"ok","timestamp":1651733102778,"user":{"displayName":"Pragnesh Solanki","userId":"07111032951740583629"},"user_tz":-330},"id":"u63RWuH55zrr","outputId":"7bfee5f8-7b6b-4a3c-9488-7a1f1f373d32"},"outputs":[{"data":{"text/plain":["0.4288235294117647"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["#Optional step: Checking accuracy on train tweets before fine-tuning so that we can see improvement by fine tuning\n","accuracy_score( y_train, np.round(model.predict(train_input)).flatten() )"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":157007,"status":"ok","timestamp":1651733259739,"user":{"displayName":"Pragnesh Solanki","userId":"07111032951740583629"},"user_tz":-330},"id":"-g9FMdev52PV","outputId":"3e72cc54-dee4-49e4-f51e-cbf6dd6976c0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/2\n","160/160 [==============================] - 1597s 10s/step - loss: 0.5594 - acc: 0.7182\n","Epoch 2/2\n","160/160 [==============================] - 1569s 10s/step - loss: 0.4015 - acc: 0.8273\n"]}],"source":["#Model Training (Fine-tuning for tweets classification)\n","epochs = 2       #Modify as neded\n","batch_size = 32  #Modify as needed\n","train_history = model.fit(train_input, y_train ,epochs= epochs,batch_size= batch_size, verbose=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":41304,"status":"ok","timestamp":1651733301007,"user":{"displayName":"Pragnesh Solanki","userId":"07111032951740583629"},"user_tz":-330},"id":"mlxUftjA79dD","outputId":"ac7f12e5-72cf-430c-e2d5-fb56023f4be3"},"outputs":[{"data":{"text/plain":["0.8433333333333334"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["#Checking accuracy on train tweets\n","accuracy_score( y_train, np.round(model.predict(train_input)) )"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23533,"status":"ok","timestamp":1651733371950,"user":{"displayName":"Pragnesh Solanki","userId":"07111032951740583629"},"user_tz":-330},"id":"TNeXXCYT-q70","outputId":"475a9f1c-9728-4fac-d49c-3a5d4bb15298"},"outputs":[{"data":{"text/plain":["0.8022284122562674"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["#Encoding test data into BERT Format. If you have  preprocessing/cleaning of input text, it should be done before this step\n","\n","test_input = encode_text_for_bert(df_test['text'].values, tokenizer_for_bert, max_len= max_len)\n","y_test = df_test['target'].values\n","\n","#Checking accuracy on test tweets. You may be able to improve it by taking bigger length of text, more epochs or by adding more dense layers into the model\n","accuracy_score( y_test, np.round(model.predict(test_input)) )"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":368,"status":"ok","timestamp":1651733375336,"user":{"displayName":"Pragnesh Solanki","userId":"07111032951740583629"},"user_tz":-330},"id":"N4AXGh2C-rxl","outputId":"9b637c23-66ff-4444-8d86-039be71496d1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Tweet Text:  We have a great data symposium today.\n","Tweet Type:  [['Non Disaster']]    Score: [[0.21351708]]\n"]}],"source":["#Running model on single text. Validating model for non-disaster text\n","\n","tweet = \"We have a great data symposium today.\"\n","\n","prediction = model.predict (  encode_text_for_bert ( [tweet], tokenizer_for_bert, max_len=max_len) ) \n","print('Tweet Text: ', tweet)\n","print ( 'Tweet Type: ', np.where(  prediction >= .5 , \"Disaster\", \"Non Disaster\" ) , '   Score:',  prediction)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":372,"status":"ok","timestamp":1651733388311,"user":{"displayName":"Pragnesh Solanki","userId":"07111032951740583629"},"user_tz":-330},"id":"Yj51NJCZ-yQS","outputId":"97bc0422-f735-48a2-900f-fe29ebb48ee3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Tweet Text:  It is a terrorist attack. Take shelter  \n","Tweet Type:  [['Disaster']]    Score: [[0.7934529]]\n"]}],"source":["#Running model on single text. Validating model for disaster text\n","\n","tweet = \"It is a terrorist attack. Take shelter  \"\n","\n","prediction = model.predict (  encode_text_for_bert ( [tweet], tokenizer_for_bert, max_len=max_len) ) \n","print('Tweet Text: ', tweet)\n","print ( 'Tweet Type: ', np.where(  prediction >= .5 , \"Disaster\", \"Non Disaster\" ) , '   Score:',  prediction)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23047,"status":"ok","timestamp":1651735267630,"user":{"displayName":"Pragnesh Solanki","userId":"07111032951740583629"},"user_tz":-330},"id":"XnJ4f84J_gbA","outputId":"df861d56-da89-4993-fc64-ccd8cae5f4b2"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 326). These functions will not be directly callable after loading.\n"]},{"name":"stdout","output_type":"stream","text":["INFO:tensorflow:Assets written to: /content/drive/MyDrive/Colab Notebooks/saved_model/BERT_Tweets_model/assets\n"]},{"name":"stderr","output_type":"stream","text":["INFO:tensorflow:Assets written to: /content/drive/MyDrive/Colab Notebooks/saved_model/BERT_Tweets_model/assets\n"]}],"source":["model.save('/content/drive/MyDrive/Colab Notebooks/saved_model/BERT_Tweets_model')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":41077,"status":"ok","timestamp":1651740780676,"user":{"displayName":"Pragnesh Solanki","userId":"07111032951740583629"},"user_tz":-330},"id":"mt8zkK6R_GqK","outputId":"ad706298-f736-4ef3-f268-206c33e4282b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"deeplearning_bert__model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_word_ids (InputLayer)    [(None, 30)]         0           []                               \n","                                                                                                  \n"," input_mask (InputLayer)        [(None, 30)]         0           []                               \n","                                                                                                  \n"," segment_ids (InputLayer)       [(None, 30)]         0           []                               \n","                                                                                                  \n"," keras_bert_layer (KerasLayer)  [(None, 768),        109482241   ['input_word_ids[0][0]',         \n","                                 (None, 30, 768)]                 'input_mask[0][0]',             \n","                                                                  'segment_ids[0][0]']            \n","                                                                                                  \n"," tf.__operators__.getitem (Slic  (None, 768)         0           ['keras_bert_layer[0][1]']       \n"," ingOpLambda)                                                                                     \n","                                                                                                  \n"," dense (Dense)                  (None, 1)            769         ['tf.__operators__.getitem[0][0]'\n","                                                                 ]                                \n","                                                                                                  \n","==================================================================================================\n","Total params: 109,483,010\n","Trainable params: 109,483,009\n","Non-trainable params: 1\n","__________________________________________________________________________________________________\n"]}],"source":["path = '/content/drive/MyDrive/Colab Notebooks/saved_model/BERT_Tweets_model'\n","\n","new_model = tf.keras.models.load_model(path)\n","\n","new_model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":235},"executionInfo":{"elapsed":374,"status":"error","timestamp":1651740785766,"user":{"displayName":"Pragnesh Solanki","userId":"07111032951740583629"},"user_tz":-330},"id":"hUDZbj8wBzcC","outputId":"39b8fb54-e869-4324-d56a-215f57e738e0"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-bd90402a85ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"It is a bad evening. Do not Go for a ride!\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m \u001b[0;34m(\u001b[0m  \u001b[0mencode_text_for_bert\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer_for_bert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tweet Text: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0;34m'Tweet Type: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0mprediction\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m.5\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m\"Disaster\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Non Disaster\"\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m'   Score:'\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'encode_text_for_bert' is not defined"]}],"source":["tweet = \"It is a bad evening. Do not Go for a ride!\"\n","\n","prediction = new_model.predict (  encode_text_for_bert ( [tweet], tokenizer_for_bert, max_len=max_len) ) \n","print('Tweet Text: ', tweet)\n","print ( 'Tweet Type: ', np.where(  prediction >= .5 , \"Disaster\", \"Non Disaster\" ) , '   Score:',  prediction)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tko4T-doCePY"},"outputs":[],"source":[""]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"BERT_Tweets_Classification.ipynb","provenance":[],"mount_file_id":"1SS6v9ZM-2xmW889f75CiuADoG3oN97If","authorship_tag":"ABX9TyNzmjlPd7xeo9ELgWPnMRT5"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}